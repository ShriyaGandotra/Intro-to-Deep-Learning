{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training with sequence_length=20, hidden_dim=64, num_layers=1, nheads=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gagan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time: 341.65 seconds\n",
      "Epoch 1, Train Loss: 2.5167428719128813, Train Accuracy: 26.32%, Val Loss: 2.464784526989095, Val Accuracy: 26.80%\n",
      "Total Training Time: 714.30 seconds\n",
      "Epoch 2, Train Loss: 2.478046365462701, Train Accuracy: 26.87%, Val Loss: 2.480775039222004, Val Accuracy: 27.05%\n",
      "Total Training Time: 1080.67 seconds\n",
      "Epoch 3, Train Loss: 2.4789787109939105, Train Accuracy: 26.87%, Val Loss: 2.4566844531467984, Val Accuracy: 27.11%\n",
      "Total Training Time: 2623.89 seconds\n",
      "Epoch 4, Train Loss: 2.4702772465302076, Train Accuracy: 26.97%, Val Loss: 2.445901823399468, Val Accuracy: 27.46%\n",
      "Total Training Time: 2961.44 seconds\n",
      "Epoch 5, Train Loss: 2.461349281065093, Train Accuracy: 27.15%, Val Loss: 2.4327595496546994, Val Accuracy: 27.65%\n",
      "Model Complexity (Number of Trainable Parameters): 125377\n",
      "Predicted next character for sequence length 20: 'e'\n",
      "Training with sequence_length=20, hidden_dim=64, num_layers=2, nheads=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gagan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time: 702.10 seconds\n",
      "Epoch 1, Train Loss: 3.321979800107762, Train Accuracy: 15.22%, Val Loss: 3.3169700111798424, Val Accuracy: 15.26%\n",
      "Total Training Time: 1405.53 seconds\n",
      "Epoch 2, Train Loss: 3.3172908794654217, Train Accuracy: 15.23%, Val Loss: 3.315158271680122, Val Accuracy: 15.26%\n",
      "Total Training Time: 12278.03 seconds\n",
      "Epoch 3, Train Loss: 3.3164612290097324, Train Accuracy: 15.23%, Val Loss: 3.3149538918329395, Val Accuracy: 15.26%\n",
      "Total Training Time: 13104.38 seconds\n",
      "Epoch 4, Train Loss: 3.316258375971603, Train Accuracy: 15.23%, Val Loss: 3.31613293312366, Val Accuracy: 15.26%\n",
      "Total Training Time: 17933.71 seconds\n",
      "Epoch 5, Train Loss: 3.316087072701271, Train Accuracy: 15.23%, Val Loss: 3.3140558304625274, Val Accuracy: 15.26%\n",
      "Model Complexity (Number of Trainable Parameters): 242113\n",
      "Predicted next character for sequence length 20: ' '\n",
      "Training with sequence_length=20, hidden_dim=128, num_layers=1, nheads=8\n",
      "Total Training Time: 247.98 seconds\n",
      "Epoch 1, Train Loss: 2.6630109104697075, Train Accuracy: 23.85%, Val Loss: 2.557589663764354, Val Accuracy: 25.34%\n",
      "Total Training Time: 417.40 seconds\n",
      "Epoch 2, Train Loss: 2.5980129800373564, Train Accuracy: 24.90%, Val Loss: 2.531764302114201, Val Accuracy: 25.81%\n",
      "Total Training Time: 583.61 seconds\n",
      "Epoch 3, Train Loss: 2.571971691961064, Train Accuracy: 25.26%, Val Loss: 2.535831835922399, Val Accuracy: 26.12%\n",
      "Total Training Time: 745.56 seconds\n",
      "Epoch 4, Train Loss: 2.5657510530380976, Train Accuracy: 25.48%, Val Loss: 2.5066608964063213, Val Accuracy: 26.75%\n",
      "Total Training Time: 916.09 seconds\n",
      "Epoch 5, Train Loss: 2.564495075487639, Train Accuracy: 25.45%, Val Loss: 2.4960100158356555, Val Accuracy: 26.72%\n",
      "Model Complexity (Number of Trainable Parameters): 480065\n",
      "Predicted next character for sequence length 20: 'l'\n",
      "Training with sequence_length=20, hidden_dim=128, num_layers=2, nheads=8\n",
      "Total Training Time: 303.48 seconds\n",
      "Epoch 1, Train Loss: 3.3236336992926385, Train Accuracy: 15.21%, Val Loss: 3.3187758696879452, Val Accuracy: 15.26%\n",
      "Total Training Time: 606.49 seconds\n",
      "Epoch 2, Train Loss: 3.3186005459937022, Train Accuracy: 15.23%, Val Loss: 3.3133732804743232, Val Accuracy: 15.26%\n",
      "Total Training Time: 894.31 seconds\n",
      "Epoch 3, Train Loss: 3.3176484682131826, Train Accuracy: 15.23%, Val Loss: 3.3174655805424718, Val Accuracy: 15.26%\n",
      "Total Training Time: 1175.98 seconds\n",
      "Epoch 4, Train Loss: 3.317344987009424, Train Accuracy: 15.23%, Val Loss: 3.314827162577924, Val Accuracy: 15.26%\n",
      "Total Training Time: 1457.67 seconds\n",
      "Epoch 5, Train Loss: 3.3171035876851347, Train Accuracy: 15.23%, Val Loss: 3.316021872059203, Val Accuracy: 15.26%\n",
      "Model Complexity (Number of Trainable Parameters): 942913\n",
      "Predicted next character for sequence length 20: ' '\n",
      "Training with sequence_length=30, hidden_dim=64, num_layers=1, nheads=8\n",
      "Total Training Time: 125.73 seconds\n",
      "Epoch 1, Train Loss: 2.5286779680336884, Train Accuracy: 26.18%, Val Loss: 2.4800433376638353, Val Accuracy: 27.09%\n",
      "Total Training Time: 250.98 seconds\n",
      "Epoch 2, Train Loss: 2.491224908274751, Train Accuracy: 26.75%, Val Loss: 2.469272843052858, Val Accuracy: 26.97%\n",
      "Total Training Time: 379.79 seconds\n",
      "Epoch 3, Train Loss: 2.480409790747342, Train Accuracy: 26.93%, Val Loss: 2.4595355889062667, Val Accuracy: 27.57%\n",
      "Total Training Time: 504.74 seconds\n",
      "Epoch 4, Train Loss: 2.477040533730851, Train Accuracy: 26.84%, Val Loss: 2.4605143197432238, Val Accuracy: 26.90%\n",
      "Total Training Time: 635.66 seconds\n",
      "Epoch 5, Train Loss: 2.477434569949469, Train Accuracy: 26.80%, Val Loss: 2.469807200916047, Val Accuracy: 26.82%\n",
      "Model Complexity (Number of Trainable Parameters): 125377\n",
      "Predicted next character for sequence length 30: 'e'\n",
      "Training with sequence_length=30, hidden_dim=64, num_layers=2, nheads=8\n",
      "Total Training Time: 243.67 seconds\n",
      "Epoch 1, Train Loss: 3.319950833685837, Train Accuracy: 15.24%, Val Loss: 3.323113126174787, Val Accuracy: 15.14%\n",
      "Total Training Time: 490.06 seconds\n",
      "Epoch 2, Train Loss: 3.316391216600618, Train Accuracy: 15.26%, Val Loss: 3.319150763274744, Val Accuracy: 15.14%\n",
      "Total Training Time: 722.15 seconds\n",
      "Epoch 3, Train Loss: 3.315327427370823, Train Accuracy: 15.26%, Val Loss: 3.3220999737409636, Val Accuracy: 15.14%\n",
      "Total Training Time: 959.30 seconds\n",
      "Epoch 4, Train Loss: 3.315004258715279, Train Accuracy: 15.26%, Val Loss: 3.318038594907409, Val Accuracy: 15.14%\n",
      "Total Training Time: 1190.58 seconds\n",
      "Epoch 5, Train Loss: 3.314890367028635, Train Accuracy: 15.26%, Val Loss: 3.3196119880512125, Val Accuracy: 15.14%\n",
      "Model Complexity (Number of Trainable Parameters): 242113\n",
      "Predicted next character for sequence length 30: ' '\n",
      "Training with sequence_length=30, hidden_dim=128, num_layers=1, nheads=8\n",
      "Total Training Time: 201.73 seconds\n",
      "Epoch 1, Train Loss: 2.642190978845781, Train Accuracy: 24.45%, Val Loss: 2.553356043169434, Val Accuracy: 25.97%\n",
      "Total Training Time: 400.76 seconds\n",
      "Epoch 2, Train Loss: 2.5915694546576415, Train Accuracy: 25.15%, Val Loss: 2.5568490208939307, Val Accuracy: 25.53%\n",
      "Total Training Time: 599.35 seconds\n",
      "Epoch 3, Train Loss: 2.6249863008829073, Train Accuracy: 24.54%, Val Loss: 2.55862002720179, Val Accuracy: 25.83%\n",
      "Total Training Time: 798.17 seconds\n",
      "Epoch 4, Train Loss: 2.598806252150582, Train Accuracy: 25.04%, Val Loss: 2.5304589171691387, Val Accuracy: 25.92%\n",
      "Total Training Time: 995.48 seconds\n",
      "Epoch 5, Train Loss: 2.5606237232719287, Train Accuracy: 25.56%, Val Loss: 2.5124859902890195, Val Accuracy: 26.51%\n",
      "Model Complexity (Number of Trainable Parameters): 480065\n",
      "Predicted next character for sequence length 30: ' '\n",
      "Training with sequence_length=30, hidden_dim=128, num_layers=2, nheads=8\n",
      "Total Training Time: 378.23 seconds\n",
      "Epoch 1, Train Loss: 3.3223745388951853, Train Accuracy: 15.25%, Val Loss: 3.319908697168785, Val Accuracy: 15.14%\n",
      "Total Training Time: 752.34 seconds\n",
      "Epoch 2, Train Loss: 3.3175320301463533, Train Accuracy: 15.26%, Val Loss: 3.3187253108601835, Val Accuracy: 15.14%\n",
      "Total Training Time: 1125.86 seconds\n",
      "Epoch 3, Train Loss: 3.3164917529826186, Train Accuracy: 15.26%, Val Loss: 3.32010061222455, Val Accuracy: 15.14%\n",
      "Total Training Time: 1499.86 seconds\n",
      "Epoch 4, Train Loss: 3.3161371650534393, Train Accuracy: 15.26%, Val Loss: 3.3201480881619303, Val Accuracy: 15.14%\n",
      "Total Training Time: 1873.02 seconds\n",
      "Epoch 5, Train Loss: 3.3161767617218536, Train Accuracy: 15.26%, Val Loss: 3.318023217114237, Val Accuracy: 15.14%\n",
      "Model Complexity (Number of Trainable Parameters): 942913\n",
      "Predicted next character for sequence length 30: ' '\n",
      "Training with sequence_length=50, hidden_dim=64, num_layers=1, nheads=8\n",
      "Total Training Time: 176.99 seconds\n",
      "Epoch 1, Train Loss: 2.543222102556624, Train Accuracy: 25.71%, Val Loss: 2.4913599566351867, Val Accuracy: 26.53%\n",
      "Total Training Time: 365.59 seconds\n",
      "Epoch 2, Train Loss: 2.5041445353643024, Train Accuracy: 26.29%, Val Loss: 2.4826729657385997, Val Accuracy: 27.17%\n",
      "Total Training Time: 544.02 seconds\n",
      "Epoch 3, Train Loss: 2.494491583886131, Train Accuracy: 26.46%, Val Loss: 2.481530912019573, Val Accuracy: 26.65%\n",
      "Total Training Time: 723.75 seconds\n",
      "Epoch 4, Train Loss: 2.4889811653092884, Train Accuracy: 26.54%, Val Loss: 2.466956924311539, Val Accuracy: 27.07%\n",
      "Total Training Time: 908.58 seconds\n",
      "Epoch 5, Train Loss: 2.484992784640485, Train Accuracy: 26.59%, Val Loss: 2.458009755549866, Val Accuracy: 27.22%\n",
      "Model Complexity (Number of Trainable Parameters): 125377\n",
      "Predicted next character for sequence length 50: 'e'\n",
      "Training with sequence_length=50, hidden_dim=64, num_layers=2, nheads=8\n",
      "Total Training Time: 330.76 seconds\n",
      "Epoch 1, Train Loss: 3.155345813493752, Train Accuracy: 16.92%, Val Loss: 3.0866469294602203, Val Accuracy: 18.09%\n",
      "Total Training Time: 687.30 seconds\n",
      "Epoch 2, Train Loss: 3.1998046647012877, Train Accuracy: 16.04%, Val Loss: 3.3127199604944875, Val Accuracy: 15.31%\n",
      "Total Training Time: 1797.51 seconds\n",
      "Epoch 3, Train Loss: 3.3169484117625863, Train Accuracy: 15.21%, Val Loss: 3.3130445712883447, Val Accuracy: 15.31%\n",
      "Total Training Time: 2220.79 seconds\n",
      "Epoch 4, Train Loss: 3.316544996753277, Train Accuracy: 15.21%, Val Loss: 3.31255125725878, Val Accuracy: 15.31%\n",
      "Total Training Time: 2745.44 seconds\n",
      "Epoch 5, Train Loss: 3.316346867767507, Train Accuracy: 15.21%, Val Loss: 3.31166985591663, Val Accuracy: 15.31%\n",
      "Model Complexity (Number of Trainable Parameters): 242113\n",
      "Predicted next character for sequence length 50: ' '\n",
      "Training with sequence_length=50, hidden_dim=128, num_layers=1, nheads=8\n",
      "Total Training Time: 381.92 seconds\n",
      "Epoch 1, Train Loss: 2.6616174436645004, Train Accuracy: 24.15%, Val Loss: 2.535539781506966, Val Accuracy: 25.95%\n",
      "Total Training Time: 1859.39 seconds\n",
      "Epoch 2, Train Loss: 2.5847915165257787, Train Accuracy: 25.08%, Val Loss: 2.5348894659021566, Val Accuracy: 26.10%\n",
      "Total Training Time: 3136.66 seconds\n",
      "Epoch 3, Train Loss: 2.580395212065385, Train Accuracy: 25.05%, Val Loss: 2.538545244668861, Val Accuracy: 26.12%\n",
      "Total Training Time: 3659.75 seconds\n",
      "Epoch 4, Train Loss: 2.5814258152093426, Train Accuracy: 25.19%, Val Loss: 2.5304360861622324, Val Accuracy: 26.36%\n",
      "Total Training Time: 4016.25 seconds\n",
      "Epoch 5, Train Loss: 2.561165744374907, Train Accuracy: 25.37%, Val Loss: 2.531800518210975, Val Accuracy: 26.65%\n",
      "Model Complexity (Number of Trainable Parameters): 480065\n",
      "Predicted next character for sequence length 50: 'h'\n",
      "Training with sequence_length=50, hidden_dim=128, num_layers=2, nheads=8\n",
      "Total Training Time: 755.71 seconds\n",
      "Epoch 1, Train Loss: 3.323811582574337, Train Accuracy: 15.20%, Val Loss: 3.313020255329003, Val Accuracy: 15.31%\n",
      "Total Training Time: 1532.66 seconds\n",
      "Epoch 2, Train Loss: 3.3189887828112776, Train Accuracy: 15.21%, Val Loss: 3.3150423146224335, Val Accuracy: 15.31%\n",
      "Total Training Time: 2307.38 seconds\n",
      "Epoch 3, Train Loss: 3.3179217890095223, Train Accuracy: 15.21%, Val Loss: 3.3148797350373, Val Accuracy: 15.31%\n",
      "Total Training Time: 3501.44 seconds\n",
      "Epoch 4, Train Loss: 3.317592655393113, Train Accuracy: 15.21%, Val Loss: 3.3147261304138587, Val Accuracy: 15.31%\n",
      "Total Training Time: 4156.88 seconds\n",
      "Epoch 5, Train Loss: 3.317444711756867, Train Accuracy: 15.21%, Val Loss: 3.315633275354312, Val Accuracy: 15.31%\n",
      "Model Complexity (Number of Trainable Parameters): 942913\n",
      "Predicted next character for sequence length 50: ' '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Character mapping to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "# Transformer Model class\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, nhead, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.transformer = nn.Transformer(hidden_dim, nhead, num_layers, num_layers, dim_feedforward=hidden_dim*4, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) * math.sqrt(hidden_dim)\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects seq_len, batch, input_dim\n",
    "        x = self.transformer(x, x)\n",
    "        x = x.permute(1, 0, 2)  # Back to batch, seq_len, input_dim\n",
    "        x = self.fc_out(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "# Function to train and validate the model\n",
    "def train_and_validate(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=5):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train_correct += (predicted == targets).sum().item()\n",
    "            total_train_samples += targets.size(0)\n",
    "\n",
    "        scheduler.step()  # Learning rate scheduler step\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in test_loader:\n",
    "                sequences, targets = sequences.to(device), targets.to(device)\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val_correct += (predicted == targets).sum().item()\n",
    "                total_val_samples += targets.size(0)\n",
    "\n",
    "        val_accuracy = 100 * total_val_correct / total_val_samples\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        print(f'Total Training Time: {training_time:.2f} seconds')\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, '\n",
    "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {total_val_loss / len(test_loader)}, '\n",
    "              f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence = [char_to_ix.get(c, 0) for c in initial_str[-max_length:]]\n",
    "        sequence = torch.tensor(sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        prediction = model(sequence)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Training configurations\n",
    "sequence_lengths = [20, 30, 50]\n",
    "hidden_dims = [64, 128]\n",
    "num_layers_list = [1, 2]\n",
    "nheads = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    sequences, targets = [], []\n",
    "    for i in range(0, len(text) - sequence_length):\n",
    "        seq = [char_to_int[ch] for ch in text[i:i+sequence_length]]\n",
    "        target = char_to_int[text[i+sequence_length]]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long).to(device)\n",
    "    targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "    dataset = CharDataset(sequences, targets)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for num_layers in num_layers_list:\n",
    "            print(f\"Training with sequence_length={sequence_length}, hidden_dim={hidden_dim}, num_layers={num_layers}, nheads={nheads}\")\n",
    "            model = TransformerModel(vocab_size, hidden_dim, num_layers, nheads).to(device)\n",
    "            model_complexity = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)  # Adjust learning rate\n",
    "            train_and_validate(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=5)\n",
    "            print(f'Model Complexity (Number of Trainable Parameters): {model_complexity}')\n",
    "            # Prediction example after training\n",
    "            test_str = \"We are accounted poor citiz\"\n",
    "            predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str, sequence_length)\n",
    "            print(f\"Predicted next character for sequence length {sequence_length}: '{predicted_char}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
